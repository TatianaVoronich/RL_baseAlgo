# RL_baseAlgo
MonteCarlo_SARSA_QLearning
## Сравнение методов.
Методы SARSA и Q-learning являются более эффективными и удобными по сравнению с методом Монте-Карло в контексте обучения с подкреплением по следующим причинам:

Онлайн обучение: SARSA и Q-learning работают в режиме онлайн, что означает, что они обновляют оценки Q-функции на каждом шаге в процессе взаимодействия с средой. В отличие от метода Монте-Карло, который требует завершения полного эпизода, методы SARSA и Q-learning обновляются по мере получения новых данных, что делает их более эффективными и подходящими для длительных эпизодов или непрерывных взаимодействий с средой.

Оценка действий: В методе Монте-Карло для оценки Q-функции требуется собирать статистику о возвращениях после каждого действия, что может быть дорого в случае длительных эпизодов или множественных вариантов действий. В методах SARSA и Q-learning оценка Q-функции происходит в режиме онлайн, и нет необходимости ждать завершения эпизода для обновления оценок.

Off-policy и On-policy: SARSA и Q-learning представляют собой различные подходы к обучению с подкреплением, которые могут быть применены как для off-policy (например, Q-learning), так и для on-policy (например, SARSA) методов управления. В то время как метод Монте-Карло обычно используется только в онлайн-подходе и может быть менее гибким для решения различных задач.

Бутстрэпинг: Как SARSA, так и Q-learning используют бутстрэпинг (bootstrapping), что означает использование собственных оценок для обновления оценок Q-функции. Это позволяет использовать текущие оценки для обновления других оценок, что может привести к более быстрой сходимости.

Оценка вероятностей: В методе Монте-Карло требуется оценить вероятности переходов между состояниями, что может быть сложной задачей для некоторых сред. Методы SARSA и Q-learning обходят эту проблему, оценивая вероятности переходов "на лету" через текущие действия и оценки Q-функции.

В целом, методы SARSA и Q-learning обладают большей гибкостью, более эффективными методами обновления оценок Q-функции и работают в режиме онлайн, что делает их предпочтительными в большинстве практических сценариев обучения с подкреплением.

Несмотря на преимущества методов SARSA и Q-learning, они также имеют свои недостатки по сравнению с методом Монте-Карло:

Смещение оценок: Методы SARSA и Q-learning используют бутстрэпинг для обновления оценок Q-функции, что может привести к смещению оценок. Это означает, что они могут быть менее стабильными в некоторых ситуациях и могут давать оценки, которые не совпадают с истинными значениями Q-функции.
Проблема исследования: Как SARSA, так и Q-learning применяют жадную стратегию выбора действий с использованием текущих оценок Q-функции. Это может привести к проблеме исследования, когда алгоритм предпочитает выбирать уже известно хорошие действия, игнорируя другие действия, которые могут привести к лучшим вознаграждениям в долгосрочной перспективе.

## Выбор оптимальной стратегии: 
В отличие от метода Монте-Карло, который обновляет оценки Q-функции только по завершении эпизода, методы SARSA и Q-learning обновляют оценки на каждом шаге внутри эпизода. Это означает, что они могут обучать Q-функцию для неполных траекторий, что может затруднить выбор оптимальной стратегии.
Проблема планирования: Методы SARSA и Q-learning не решают проблему планирования, т.е. способности выбирать наилучшие действия в неизвестных состояниях. Они обновляют Q-функцию только на основе текущих наблюдений и вознаграждений, но не строят планы на будущее.
Обновление весов: Параметры методов SARSA и Q-learning обновляются на каждом шаге, что может привести к колебаниям и нестабильности обучения. В методе Монте-Карло веса обновляются только после завершения эпизода, что может сделать обучение более стабильным.
Выбор между методами SARSA, Q-learning и Monte-Carlo зависит от конкретной задачи обучения с подкреплением и характеристик среды. Вот некоторые рекомендации о том, в каких случаях выбрать каждый метод:

SARSA: SARSA является on-policy методом, что означает, что он обновляет оценки Q-функции на основе текущей стратегии. В таких ситуациях, когда у вас есть ограниченное количество итераций или желание сохранить стабильность обучения, SARSA может быть предпочтительным выбором. Если у вас есть проблема исследования (например, в среде с множеством неизвестных состояний и действий), SARSA может быть полезным, так как он использует текущую стратегию и имеет меньшую вероятность выбирать оптимальные действия сразу, что может помочь исследовать среду.

Q-learning: Q-learning является off-policy методом, что означает, что он обновляет оценки Q-функции, используя жадную стратегию, но выбирает действия для обучения, исходя из другой стратегии (например, epsilon-жадной стратегии). Если у вас есть информация о наилучшей стратегии или есть возможность исследовать окружение с использованием других стратегий, Q-learning может быть предпочтительным выбором. Когда у вас есть много данных и необходимость обучать на большом объеме информации, Q-learning может быть более эффективным, так как он обновляет Q-функцию в режиме онлайн и может быстрее сходиться.

Monte-Carlo: Метод Monte-Carlo часто используется, когда у вас есть длительные эпизоды и вы не можете себе позволить делать обновления Q-функции на каждом шаге. В этом случае Monte-Carlo может быть более эффективным, так как он обновляет Q-функцию только после завершения эпизода. Если ваша задача связана с оценкой вероятностей или оценкой ценности состояний, метод Monte-Carlo может быть более подходящим, так как он не требует априорной информации о вероятностях переходов и вознаграждениях.
